{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_data(data):    \n",
    "    # Entferne aus den Trainingsdaten die Zeilen, die um 23 Uhr sind, weil wir nur an Samstag Sonntag bis 23 Uhr offen haben, das aber unsere\n",
    "    # Periodizität kaputt macht\n",
    "    data = data[data[\"hour\"] != 23]\n",
    "\n",
    "    #Entferne die Spalten \"year\", \"week\", \"coronaImpact\", weil diese keinen Impact haben\n",
    "    data = data.drop(columns=[\"year\", \"week\", \"coronaImpact\"])\n",
    "\n",
    "    # Sinus and Cosinus Transformation for \"hour\" and \"dayOfMonth\"\n",
    "    # data[\"hour_sin\"] = np.sin(2 * np.pi * data[\"hour\"] / 24)\n",
    "    # data[\"hour_cos\"] = np.cos(2 * np.pi * data[\"hour\"] / 24)\n",
    "    # data[\"dayOfMonth_sin\"] = np.sin(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    # data[\"dayOfMonth_cos\"] = np.cos(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    # data = data.drop(columns=[\"hour\", \"dayOfMonth\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "def split_train_test_data(data, target_col=\"revenue\"):\n",
    "    # date in datetime umwandeln\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "\n",
    "    # Auswählen des Testdatensatzes (letzter Monat)\n",
    "    last_month = data['date'].dt.month.max()\n",
    "    test_data = data[data['date'].dt.month == last_month]\n",
    "    train_data = data[data['date'].dt.month != last_month]\n",
    "\n",
    "    # Reduziere die Trainingsdaten auf die letzten 2 Jahre\n",
    "    last_date = train_data[\"date\"].max()\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    last_date = last_date - pd.DateOffset(years=2)\n",
    "    train_data = train_data[pd.to_datetime(train_data.date) > last_date]\n",
    "\n",
    "    # Entferne Spalte \"date\"\n",
    "    train_data = train_data.drop(columns=[\"date\"])\n",
    "    test_data = test_data.drop(columns=[\"date\"])\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "    # x, y trennen\n",
    "    # X_train = train_data.drop(columns=[target_col])\n",
    "    # y_train = train_data[target_col]\n",
    "    # X_test = test_data.drop(columns=[target_col])\n",
    "    # y_test = test_data[target_col]\n",
    "\n",
    "    # return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data_till_2023.csv\")\n",
    "\n",
    "train_data = preprocess_data(train_data)\n",
    "train_data.to_csv(\"train_data_till_2023_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-03-09 21:55:30,972] A new study created in memory with name: no-name-eb9166c0-8113-40a3-bac7-255db6fb7ba9\n",
      "[W 2024-03-09 21:59:05,596] Trial 0 failed with parameters: {'use_RBF': 1, 'length_scale_RBF': 0.39609252380096555, 'use_WhiteKernel': 1, 'noise_level_WhiteKernel': 13.527684959285995, 'use_RationalQuadratic': 1, 'length_scale_RationalQuadratic': 0.0023366099412723555, 'alpha_RationalQuadratic': 0.060521280345881, 'use_ExpSineSquared': 1, 'length_scale_ExpSineSquared': 0.5116120480590125, 'periodicity_ExpSineSquared': 0.00015876594548668886, 'use_DotProduct': 0} because of the following error: NameError(\"name 'val_data' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_495460/1667466441.py\", line 47, in objective\n",
      "    X_val = np.column_stack([val_data['day_x_hour'], val_data['hour_sin'], val_data['hour_cos'],\n",
      "NameError: name 'val_data' is not defined\n",
      "[W 2024-03-09 21:59:05,639] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Führe die Optimierung mit Optuna durch\u001b[39;00m\n\u001b[1;32m     56\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Extrahiere die besten Hyperparameter und Kernelzusammensetzung\u001b[39;00m\n\u001b[1;32m     60\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     44\u001b[0m gp \u001b[38;5;241m=\u001b[39m GaussianProcessRegressor(kernel\u001b[38;5;241m=\u001b[39mkernel, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, normalize_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m gp\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 47\u001b[0m X_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack([\u001b[43mval_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_x_hour\u001b[39m\u001b[38;5;124m'\u001b[39m], val_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour_sin\u001b[39m\u001b[38;5;124m'\u001b[39m], val_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour_cos\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     48\u001b[0m                          val_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdayOfMonth_sin\u001b[39m\u001b[38;5;124m'\u001b[39m], val_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdayOfMonth_cos\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     49\u001b[0m y_val \u001b[38;5;241m=\u001b[39m (val_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevenue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m val_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevenue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m val_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevenue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstd()\n\u001b[1;32m     51\u001b[0m y_pred, _ \u001b[38;5;241m=\u001b[39m gp\u001b[38;5;241m.\u001b[39mpredict(X_val, return_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared, DotProduct\n",
    "import optuna\n",
    "\n",
    "# Vorverarbeitung deiner Daten\n",
    "X_train = np.column_stack([train_data['day_x_hour'], train_data['hour_sin'], train_data['hour_cos'], \n",
    "                           train_data['dayOfMonth_sin'], train_data['dayOfMonth_cos']])\n",
    "y_train = (train_data['revenue'] - train_data['revenue'].mean()) / train_data['revenue'].std()\n",
    "\n",
    "# Definiere die Kernelkomponenten\n",
    "kernel_components = [\n",
    "    RBF(length_scale=1.0, length_scale_bounds=(1e-5, 100.0)),\n",
    "    WhiteKernel(noise_level=1.0, noise_level_bounds=(1e-5, 100.0)),\n",
    "    RationalQuadratic(length_scale=1.0, alpha=1.0, length_scale_bounds=(1e-5, 100.0), alpha_bounds=(1e-5, 100.0)),\n",
    "    ExpSineSquared(length_scale=1.0, periodicity=1.0, length_scale_bounds=(1e-5, 100.0), periodicity_bounds=(1e-5, 100.0)),\n",
    "    DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-5, 100.0))\n",
    "]\n",
    "\n",
    "# Optuna-Objektiv-Funktion\n",
    "def objective(trial):\n",
    "    kernel = 1.0\n",
    "    for component in kernel_components:\n",
    "        if trial.suggest_categorical(f\"use_{component.__class__.__name__}\", [0, 1]):\n",
    "            if isinstance(component, RBF):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(length_scale=length_scale)\n",
    "            elif isinstance(component, WhiteKernel):\n",
    "                noise_level = trial.suggest_float(f\"noise_level_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(noise_level=noise_level)\n",
    "            elif isinstance(component, RationalQuadratic):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                alpha = trial.suggest_float(f\"alpha_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(length_scale=length_scale, alpha=alpha)\n",
    "            elif isinstance(component, ExpSineSquared):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                periodicity = trial.suggest_float(f\"periodicity_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(length_scale=length_scale, periodicity=periodicity)\n",
    "            elif isinstance(component, DotProduct):\n",
    "                sigma_0 = trial.suggest_float(f\"sigma_0_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(sigma_0=sigma_0)\n",
    "            kernel *= component\n",
    "        \n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=True)\n",
    "    gp.fit(X_train, y_train)\n",
    "    \n",
    "    X_val = np.column_stack([val_data['day_x_hour'], val_data['hour_sin'], val_data['hour_cos'],\n",
    "                             val_data['dayOfMonth_sin'], val_data['dayOfMonth_cos']])\n",
    "    y_val = (val_data['revenue'] - val_data['revenue'].mean()) / val_data['revenue'].std()\n",
    "    \n",
    "    y_pred, _ = gp.predict(X_val, return_std=False)\n",
    "    score = -np.mean((y_val - y_pred) ** 2)  # Negatives durchschnittliches quadratisches Fehler\n",
    "    return score\n",
    "\n",
    "# Führe die Optimierung mit Optuna durch\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Extrahiere die besten Hyperparameter und Kernelzusammensetzung\n",
    "best_params = study.best_trial.params\n",
    "kernel = 1.0\n",
    "for component in kernel_components:\n",
    "    if best_params.get(f\"use_{component.__class__.__name__}\", 0):\n",
    "        component_params = {param: best_params[param] for param in best_params if param.startswith(f\"{component.__class__.__name__}\")}\n",
    "        component.set_params(**component_params)\n",
    "        kernel *= component\n",
    "\n",
    "# Trainiere das endgültige Modell mit den besten Hyperparametern\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=True)\n",
    "gp.fit(X_train, y_train)\n",
    "\n",
    "# Mache Vorhersagen auf Testdaten\n",
    "X_test = np.column_stack([test_data['day_x_hour'], test_data['hour_sin'], test_data['hour_cos'],\n",
    "                          test_data['dayOfMonth_sin'], test_data['dayOfMonth_cos']])\n",
    "y_pred, _ = gp.predict(X_test, return_std=False)\n",
    "y_pred = y_pred * test_data['revenue'].std() + test_data['revenue'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GPy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mGPy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm, gamma\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymc3\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpm\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'GPy'"
     ]
    }
   ],
   "source": [
    "import GPy\n",
    "from scipy.stats import norm, gamma\n",
    "\n",
    "# Definiere die Kernel-Komposition\n",
    "kernel = GPy.kern.Sum(\n",
    "    GPy.kern.Periodic(1, period=10, lengthscale=1.0, lower=1e-5, upper=100.0),  # Täglicher Zyklus (10 Stunden)\n",
    "    GPy.kern.Periodic(1, period=70, lengthscale=1.0, lower=1e-5, upper=100.0), # Wöchentlicher Zyklus (10 Stunden * 7 Tage)\n",
    "    GPy.kern.Linear(),  \n",
    "    GPy.kern.RBF(1, lengthscale=1.0, lower=1e-5, upper=100.0),\n",
    "    GPy.kern.Spectral1D(1, lengthscale=1.0, lower=1e-5, upper=100.0), \n",
    "    GPy.kern.Spectral1D(1, lengthscale=1.0, lower=1e-5, upper=100.0)\n",
    ")\n",
    "\n",
    "num_timeseries = len(train_data) // 100 * 3 # Verwende z.B. 3% der Daten für das hierarchische Modell\n",
    "with pm.Model() as hier_model:\n",
    "    # Hyperpriors\n",
    "    nu_s = pm.Normal('nu_s', mu=0, sigma=5)\n",
    "    nu_r = pm.Normal('nu_r', mu=0, sigma=5)\n",
    "    nu_p1 = pm.Normal('nu_p1', mu=0, sigma=5)\n",
    "    nu_p2 = pm.Normal('nu_p2', mu=0, sigma=5)\n",
    "    nu_m1 = pm.Normal('nu_m1', mu=-1.5, sigma=5)\n",
    "    nu_m2 = pm.Normal('nu_m2', mu=0, sigma=5)\n",
    "    lambda_s = pm.Gamma('lambda_s', alpha=1, beta=1)\n",
    "    lambda_l = pm.Gamma('lambda_l', alpha=1, beta=1)\n",
    "\n",
    "    # Hierarchical priors\n",
    "    sigma_s = pm.LogNormal('sigma_s', nu_s, lambda_s, shape=num_timeseries)\n",
    "    l_rbf = pm.LogNormal('l_rbf', nu_r, lambda_l, shape=num_timeseries)\n",
    "    l_per1 = pm.LogNormal('l_per1', nu_p1, lambda_l, shape=num_timeseries)\n",
    "    l_per2 = pm.LogNormal('l_per2', nu_p2, lambda_l, shape=num_timeseries)\n",
    "    l_sm1 = pm.LogNormal('l_sm1', nu_m1, lambda_l, shape=num_timeseries)\n",
    "    l_sm2 = pm.LogNormal('l_sm2', nu_m2, lambda_l, shape=num_timeseries)\n",
    "\n",
    "    # Likelihood (Beispiel für eine Normalverteilung)\n",
    "    y_obs = pm.Normal('y_obs', mu=0, sigma=sigma_s, observed=y_train)\n",
    "\n",
    "    # Inferenz\n",
    "    trace = pm.sample(1000, chains=2, cores=2)\n",
    "\n",
    "# Extrahiere die a-posteriori Verteilungen der Hyperparameter\n",
    "nu_s_post = trace['nu_s'].mean()\n",
    "nu_r_post = trace['nu_r'].mean()\n",
    "nu_p1_post = trace['nu_p1'].mean()\n",
    "nu_p2_post = trace['nu_p2'].mean()\n",
    "nu_m1_post = trace['nu_m1'].mean()\n",
    "nu_m2_post = trace['nu_m2'].mean()\n",
    "lambda_s_post = trace['lambda_s'].mean()\n",
    "lambda_l_post = trace['lambda_l'].mean()\n",
    "\n",
    "print(nu_s_post, nu_r_post, nu_p1_post, nu_p2_post, nu_m1_post, nu_m2_post, lambda_s_post, lambda_l_post)\n",
    "\n",
    "# Erstelle das GP-Modell mit den Priors\n",
    "gp = GPy.models.GPRegression(X_train, y_train, kernel=kernel)\n",
    "gp.kern.rbf.lengthscale.unconstrain()\n",
    "gp.kern.periodic.lengthscales.unconstrain()\n",
    "gp.kern.spectral1d_1.lengthscale.unconstrain()\n",
    "gp.kern.spectral1d_2.lengthscale.unconstrain()\n",
    "\n",
    "# Setze die Priors mit den geschätzten Werten\n",
    "gp.kern.rbf.lengthscale.set_prior(norm(nu_r_post, lambda_l_post))\n",
    "gp.kern.periodic.lengthscales.set_prior(norm([nu_p1_post, nu_p2_post], lambda_l_post))\n",
    "gp.kern.spectral1d_1.lengthscale .set_prior(norm(nu_m1_post, lambda_l_post))\n",
    "gp.kern.spectral1d_2.lengthscale.set_prior(norm(nu_m2_post, lambda_l_post))\n",
    "\n",
    "gp.kern.rbf.variance.set_prior(gamma(shape=lambda_s_post, scale=1/nu_s_post))\n",
    "gp.kern.periodic.variance.set_prior(gamma(shape=lambda_s_post, scale=1/nu_s_post))\n",
    "gp.kern.linear.variances.set_prior(gamma(shape=lambda_s_post, scale=1/nu_s_post))\n",
    "gp.kern.spectral1d_1.variance.set_prior(gamma(shape=lambda_s_post, scale=1/nu_s_post))\n",
    "gp.kern.spectral1d_2.variance.set_prior(gamma(shape=lambda_s_post, scale=1/nu_s_post))\n",
    "\n",
    "# Trainiere das GP-Modell\n",
    "gp.optimize()\n",
    "\n",
    "# Mache Vorhersagen\n",
    "y_pred, y_var = gp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Speichere das Kernel-Objekt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimized_kernel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mgp\u001b[49m\u001b[38;5;241m.\u001b[39mkern, file)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRBF Kernel:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengthscale: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgp\u001b[38;5;241m.\u001b[39mkern\u001b[38;5;241m.\u001b[39mrbf\u001b[38;5;241m.\u001b[39mlengthscale\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gp' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Speichere das Kernel-Objekt\n",
    "with open('optimized_kernel.pkl', 'wb') as file:\n",
    "    pickle.dump(gp.kern, file)\n",
    "\n",
    "\n",
    "print(\"RBF Kernel:\")\n",
    "print(f\"Lengthscale: {gp.kern.rbf.lengthscale.values}\")\n",
    "print(f\"Variance: {gp.kern.rbf.variance.values}\")\n",
    "\n",
    "print(\"Periodischer Kernel 1 (täglich):\")\n",
    "print(f\"Periode: {gp.kern.periodic.period}\")  \n",
    "print(f\"Lengthscale: {gp.kern.periodic.lengthscale.values}\")\n",
    "print(f\"Variance: {gp.kern.periodic.variance.values}\")\n",
    "print(\"Periodischer Kernel 2 (wöchentlich):\")\n",
    "print(f\"Periode: {gp.kern.periodic.period}\")\n",
    "print(f\"Lengthscale: {gp.kern.periodic.lengthscale.values}\")\n",
    "print(f\"Variance: {gp.kern.periodic.variance.values}\")\n",
    "print(\"Linear Kernel:\")\n",
    "print(f\"Variances: {gp.kern.linear.variances.values}\")\n",
    "print(\"Spectral Kernel 1:\")\n",
    "print(f\"Lengthscale: {gp.kern.spectral1d_1.lengthscale.values}\")\n",
    "print(f\"Variance: {gp.kern.spectral1d_1.variance.values}\")\n",
    "print(\"Spectral Kernel 2:\")\n",
    "print(f\"Lengthscale: {gp.kern.spectral1d_2.lengthscale.values}\")\n",
    "\n",
    "\n",
    "# Laden Sie es später wieder ein\n",
    "# with open('optimized_kernel.pkl', 'rb') as file:\n",
    "#     optimized_kernel = pickle.load(file)\n",
    "\n",
    "# # Verwenden Sie das geladene Kernel in einem neuen GPy-Modell\n",
    "# new_gp = GPy.models.GPRegression(X_train, y_train, kernel=optimized_kernel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-03-10 17:43:16,963] A new study created in memory with name: no-name-7d279b24-f2c5-46be-b9b0-ac05c7b70e31\n",
      "[I 2024-03-10 17:46:45,378] Trial 0 finished with value: -434177.04080032464 and parameters: {'use_WhiteKernel': 1, 'noise_level_WhiteKernel': 0.0014986089319315565, 'use_RationalQuadratic': 1, 'length_scale_RationalQuadratic': 1.4640211525479963e-05, 'alpha_RationalQuadratic': 8.236409913565002, 'use_ExpSineSquared': 0, 'use_DotProduct': 0}. Best is trial 0 with value: -434177.04080032464.\n",
      "[W 2024-03-10 17:46:57,260] Trial 1 failed with parameters: {'use_WhiteKernel': 0, 'use_RationalQuadratic': 0, 'use_ExpSineSquared': 1, 'length_scale_ExpSineSquared': 0.025046943972269153, 'use_DotProduct': 0} because of the following error: LinAlgError(\"The kernel, 1**2 * ExpSineSquared(length_scale=0.025, periodicity=10) * ExpSineSquared(length_scale=0.025, periodicity=70), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '130-th leading minor of the array is not positive definite').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_49808/537414622.py\", line 115, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=100, n_jobs=1)\n",
      "  File \"/tmp/ipykernel_49808/537414622.py\", line 70, in objective\n",
      "    gp.fit(X_train, y_train)\n",
      "  File \"/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py\", line 345, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/scipy/linalg/_decomp_cholesky.py\", line 89, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/scipy/linalg/_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, 1**2 * ExpSineSquared(length_scale=0.025, periodicity=10) * ExpSineSquared(length_scale=0.025, periodicity=70), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '130-th leading minor of the array is not positive definite')\n",
      "[W 2024-03-10 17:46:57,261] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "(\"The kernel, 1**2 * ExpSineSquared(length_scale=0.025, periodicity=10) * ExpSineSquared(length_scale=0.025, periodicity=70), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '130-th leading minor of the array is not positive definite')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 115\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_train, y_train, X_val, y_val \u001b[38;5;129;01min\u001b[39;00m train_val_splits:\n\u001b[1;32m    114\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     score \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_value\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m best_score:\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[1], line 115\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_train, y_train, X_val, y_val \u001b[38;5;129;01min\u001b[39;00m train_val_splits:\n\u001b[1;32m    114\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    117\u001b[0m     score \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_value\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m best_score:\n",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     67\u001b[0m         kernel \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m component\n\u001b[1;32m     69\u001b[0m gp \u001b[38;5;241m=\u001b[39m GaussianProcessRegressor(kernel\u001b[38;5;241m=\u001b[39mkernel, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m \u001b[43mgp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m gp\u001b[38;5;241m.\u001b[39mpredict(X_val, return_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     73\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean((y_val \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Negatives durchschnittliches quadratisches Fehler\u001b[39;00m\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py:345\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    343\u001b[0m K[np\u001b[38;5;241m.\u001b[39mdiag_indices_from(K)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL_ \u001b[38;5;241m=\u001b[39m \u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGPR_CHOLESKY_LOWER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mLinAlgError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    347\u001b[0m     exc\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    348\u001b[0m         (\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe kernel, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is not returning a positive \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m         ),\n\u001b[1;32m    353\u001b[0m     ) \u001b[38;5;241m+\u001b[39m exc\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/scipy/linalg/_decomp_cholesky.py:89\u001b[0m, in \u001b[0;36mcholesky\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(a, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    Compute the Cholesky decomposition of a matrix.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     c, lower \u001b[38;5;241m=\u001b[39m \u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/scipy/linalg/_decomp_cholesky.py:37\u001b[0m, in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     35\u001b[0m c, info \u001b[38;5;241m=\u001b[39m potrf(a1, lower\u001b[38;5;241m=\u001b[39mlower, overwrite_a\u001b[38;5;241m=\u001b[39moverwrite_a, clean\u001b[38;5;241m=\u001b[39mclean)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-th leading minor of the array is not positive \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefinite\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m info)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAPACK reported an illegal value in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-th argument\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon entry to \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOTRF\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m-\u001b[39minfo))\n",
      "\u001b[0;31mLinAlgError\u001b[0m: (\"The kernel, 1**2 * ExpSineSquared(length_scale=0.025, periodicity=10) * ExpSineSquared(length_scale=0.025, periodicity=70), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '130-th leading minor of the array is not positive definite')"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared, DotProduct\n",
    "import optuna\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Reduziere die Trainingsdaten auf die letzten 4 Jahre\n",
    "    last_date = data[\"date\"].max()\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    last_date = last_date - pd.DateOffset(years=4)\n",
    "    data = data[pd.to_datetime(data.date) > last_date]\n",
    "\n",
    "    \n",
    "    # Entferne aus den Trainingsdaten die Zeilen, die um 23 Uhr sind, weil wir nur an Samstag Sonntag bis 23 Uhr offen haben, das aber unsere\n",
    "    # Periodizität kaputt macht\n",
    "    data = data[data[\"hour\"] != 23]\n",
    "\n",
    "    #Entferne die Spalten \"year\", \"week\", \"coronaImpact\", weil diese keinen Impact haben\n",
    "    data = data.drop(columns=[\"year\", \"week\", \"coronaImpact\"])\n",
    "\n",
    "    # Sinus and Cosinus Transformation for \"hour\" and \"dayOfMonth\"\n",
    "    data[\"hour_sin\"] = np.sin(2 * np.pi * data[\"hour\"] / 24)\n",
    "    data[\"hour_cos\"] = np.cos(2 * np.pi * data[\"hour\"] / 24)\n",
    "    data[\"dayOfMonth_sin\"] = np.sin(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    data[\"dayOfMonth_cos\"] = np.cos(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    data = data.drop(columns=[\"hour\", \"dayOfMonth\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Vorverarbeitung deiner Daten\n",
    "train_data = pd.read_csv(\"train_data_till_2023.csv\")\n",
    "train_data = preprocess_data(train_data)\n",
    "\n",
    "# Definiere die Kernelkomponenten\n",
    "kernel_components = [\n",
    "    # RBF(length_scale=1.0, length_scale_bounds=(1e-5, 100.0)),\n",
    "    WhiteKernel(noise_level=1.0, noise_level_bounds=(1e-5, 100.0)),\n",
    "    RationalQuadratic(length_scale=1.0, alpha=1.0, length_scale_bounds=(1e-5, 100.0), alpha_bounds=(1e-5, 100.0)),\n",
    "    ExpSineSquared(length_scale=1.0, periodicity=10.0, length_scale_bounds=(1e-5, 100.0), periodicity_bounds=(1e-5, 100.0)),\n",
    "    ExpSineSquared(length_scale=1.0, periodicity=70.0, length_scale_bounds=(1e-5, 100.0), periodicity_bounds=(1e-5, 100.0)),\n",
    "    DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-5, 100.0))\n",
    "]\n",
    "\n",
    "# Optuna-Objektiv-Funktion\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    kernel = 1.0\n",
    "    for component in kernel_components:\n",
    "        if trial.suggest_categorical(f\"use_{component.__class__.__name__}\", [0, 1]):\n",
    "            if isinstance(component, RBF):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(length_scale=length_scale)\n",
    "            elif isinstance(component, WhiteKernel):\n",
    "                noise_level = trial.suggest_float(f\"noise_level_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(noise_level=noise_level)\n",
    "            elif isinstance(component, RationalQuadratic):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                alpha = trial.suggest_float(f\"alpha_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(length_scale=length_scale, alpha=alpha)\n",
    "            elif isinstance(component, ExpSineSquared):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                periodicity = component.periodicity  # Keine Optimierung der Periodizität\n",
    "                component.set_params(length_scale=length_scale)\n",
    "            elif isinstance(component, DotProduct):\n",
    "                sigma_0 = trial.suggest_float(f\"sigma_0_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(sigma_0=sigma_0)\n",
    "            kernel *= component\n",
    "        \n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0)\n",
    "    gp.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gp.predict(X_val, return_std=False)\n",
    "    score = -np.mean((y_val - y_pred) ** 2)  # Negatives durchschnittliches quadratisches Fehler\n",
    "    return score\n",
    "\n",
    "# Erstelle mehrere Train-Validierungs-Splits\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "# Finde den Startmonat und das Startjahr\n",
    "start_year = train_data['date'].dt.year.min()\n",
    "start_month = train_data['date'][train_data['date'].dt.year == start_year].dt.month.min()\n",
    "\n",
    "# Berechne den absoluten Monat für jeden Eintrag\n",
    "train_data['abs_month'] = (train_data['date'].dt.year - start_year) * 12 + (train_data['date'].dt.month - start_month)\n",
    "\n",
    "num_splits = 5\n",
    "train_val_splits = []\n",
    "\n",
    "for i in range(num_splits):\n",
    "    # Da wir die ersten 19 Monate nutzen wollen, adjustiere die Grenzen entsprechend\n",
    "    train_months_end = 18 + i # Der 19. Monat (0-basiert)\n",
    "    val_month = 19 + i  # Der Validierungsmonat folgt auf die ersten 19 Monate\n",
    "\n",
    "    # Filtere die Daten\n",
    "    train_data_split = train_data[train_data['abs_month'] <= train_months_end]\n",
    "    val_data_split = train_data[train_data['abs_month'] == val_month]\n",
    "\n",
    "    # Entferne die Hilfsspalten für die Aufteilung\n",
    "    train_data_split = train_data_split.drop(columns=[\"date\", \"abs_month\"])\n",
    "    val_data_split = val_data_split.drop(columns=[\"date\", \"abs_month\"])\n",
    "\n",
    "    # Teile die Daten in Features und Zielvariable\n",
    "    X_train = train_data_split.drop(columns=[\"revenue\"])\n",
    "    y_train = train_data_split[\"revenue\"]\n",
    "    X_val = val_data_split.drop(columns=[\"revenue\"])\n",
    "    y_val = val_data_split[\"revenue\"]\n",
    "\n",
    "    train_val_splits.append((X_train, y_train, X_val, y_val))\n",
    "\n",
    "# Führe die Optimierung mit Optuna durch\n",
    "best_params = None\n",
    "best_score = -np.inf\n",
    "for X_train, y_train, X_val, y_val in train_val_splits:\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=100, n_jobs=1)\n",
    "    \n",
    "    score = study.best_value\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = study.best_trial.params\n",
    "\n",
    "# Extrahiere die besten Hyperparameter und Kernelzusammensetzung\n",
    "kernel = 1.0\n",
    "for component in kernel_components:\n",
    "    if best_params.get(f\"use_{component.__class__.__name__}\", 0):\n",
    "        component_params = {param: best_params[param] for param in best_params if param.startswith(f\"{component.__class__.__name__}\")}\n",
    "        component.set_params(**component_params)\n",
    "        kernel *= component\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gebe die besten Hyperparameter und Kernelzusammensetzung aus\n",
    "print(\"Beste Hyperparameter:\")\n",
    "print(best_params)\n",
    "print(\"Beste Kernelzusammensetzung:\")\n",
    "print(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Trainiere das endgültige Modell mit den besten Hyperparametern\n",
    "# X_train = np.column_stack([train_data['day_x_hour'], train_data['hour_sin'], train_data['hour_cos'],\n",
    "#                            train_data['dayOfMonth_sin'], train_data['dayOfMonth_cos']])\n",
    "# y_train = (train_data['revenue'] - train_data['revenue'].mean()) / train_data['revenue'].std()\n",
    "\n",
    "# gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=True)\n",
    "# gp.fit(X_train, y_train)\n",
    "\n",
    "# # Mache Vorhersagen auf Testdaten\n",
    "# test_data = train_data[train_data['date'].dt.month == train_data['date'].dt.month.max()]\n",
    "# X_test = np.column_stack([test_data['day_x_hour'], test_data['hour_sin'], test_data['hour_cos'],\n",
    "#                           test_data['dayOfMonth_sin'], test_data['dayOfMonth_cos']])\n",
    "# y_pred, _ = gp.predict(X_test, return_std=False)\n",
    "# y_pred = y_pred * test_data['revenue'].std() + test_data['revenue'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-03-10 18:54:58,017] A new study created in memory with name: no-name-52dfaa71-84f7-4f60-ae09-b443489a3f9d\n",
      "[W 2024-03-10 18:54:58,024] Trial 0 failed with parameters: {'use_StdPeriodic': 0, 'use_Linear': 1, 'use_RBF': 1, 'lengthscale_RBF': 1.2915675217069649} because of the following error: AssertionError().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_70759/3252712527.py\", line 107, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=100, n_jobs=1)\n",
      "  File \"/tmp/ipykernel_70759/3252712527.py\", line 60, in objective\n",
      "    gp = GPy.models.GPRegression(X_train, y_train, kernel=kernel)\n",
      "  File \"/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/paramz/parameterized.py\", line 53, in __call__\n",
      "    self = super(ParametersChangedMeta, self).__call__(*args, **kw)\n",
      "  File \"/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/GPy/models/gp_regression.py\", line 36, in __init__\n",
      "    super(GPRegression, self).__init__(X, Y, kernel, likelihood, name='GP regression', Y_metadata=Y_metadata, normalizer=normalizer, mean_function=mean_function)\n",
      "  File \"/home/tc/vscode/BA_XAI/.venv/lib/python3.10/site-packages/GPy/core/gp.py\", line 46, in __init__\n",
      "    assert Y.ndim == 2\n",
      "AssertionError\n",
      "[W 2024-03-10 18:54:58,025] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end= [<GPy.kern.src.linear.Linear object at 0x7fae0c4a01f0>, <GPy.kern.src.rbf.RBF object at 0x7fadc2d2f4c0>]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_train, y_train, X_val, y_val \u001b[38;5;129;01min\u001b[39;00m train_val_splits:\n\u001b[1;32m    106\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     score \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_value\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m best_score:\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_train, y_train, X_val, y_val \u001b[38;5;129;01min\u001b[39;00m train_val_splits:\n\u001b[1;32m    106\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    109\u001b[0m     score \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_value\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m best_score:\n",
      "Cell \u001b[0;32mIn[1], line 60\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kernel_list[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m     58\u001b[0m     kernel \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m k\n\u001b[0;32m---> 60\u001b[0m gp \u001b[38;5;241m=\u001b[39m \u001b[43mGPy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m gp\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[1;32m     63\u001b[0m y_pred, y_var \u001b[38;5;241m=\u001b[39m gp\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/paramz/parameterized.py:53\u001b[0m, in \u001b[0;36mParametersChangedMeta.__call__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#import ipdb;ipdb.set_trace()\u001b[39;00m\n\u001b[1;32m     52\u001b[0m initialize \u001b[38;5;241m=\u001b[39m kw\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialize\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mParametersChangedMeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#logger.debug(\"finished init\")\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_init_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/GPy/models/gp_regression.py:36\u001b[0m, in \u001b[0;36mGPRegression.__init__\u001b[0;34m(self, X, Y, kernel, Y_metadata, normalizer, noise_var, mean_function)\u001b[0m\n\u001b[1;32m     32\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m kern\u001b[38;5;241m.\u001b[39mRBF(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     34\u001b[0m likelihood \u001b[38;5;241m=\u001b[39m likelihoods\u001b[38;5;241m.\u001b[39mGaussian(variance\u001b[38;5;241m=\u001b[39mnoise_var)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGPRegression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGP regression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vscode/BA_XAI/.venv/lib/python3.10/site-packages/GPy/core/gp.py:46\u001b[0m, in \u001b[0;36mGP.__init__\u001b[0;34m(self, X, Y, kernel, likelihood, mean_function, inference_method, name, Y_metadata, normalizer)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m ObsAr(X)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     47\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitializing Y\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import GPy\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Reduziere die Trainingsdaten auf die letzten 4 Jahre\n",
    "    last_date = data[\"date\"].max()\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    last_date = last_date - pd.DateOffset(months=25)\n",
    "    data = data[pd.to_datetime(data.date) > last_date]\n",
    "\n",
    "    \n",
    "    # Entferne aus den Trainingsdaten die Zeilen, die um 23 Uhr sind, weil wir nur an Samstag Sonntag bis 23 Uhr offen haben, das aber unsere\n",
    "    # Periodizität kaputt macht\n",
    "    data = data[data[\"hour\"] != 23]\n",
    "\n",
    "    #Entferne die Spalten \"year\", \"week\", \"coronaImpact\", weil diese keinen Impact haben\n",
    "    data = data.drop(columns=[\"year\", \"week\", \"coronaImpact\"])\n",
    "\n",
    "    # Sinus and Cosinus Transformation for \"hour\" and \"dayOfMonth\"\n",
    "    data[\"hour_sin\"] = np.sin(2 * np.pi * data[\"hour\"] / 24)\n",
    "    data[\"hour_cos\"] = np.cos(2 * np.pi * data[\"hour\"] / 24)\n",
    "    data[\"dayOfMonth_sin\"] = np.sin(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    data[\"dayOfMonth_cos\"] = np.cos(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    data = data.drop(columns=[\"hour\", \"dayOfMonth\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Vorverarbeitung deiner Daten\n",
    "train_data = pd.read_csv(\"train_data_till_2023.csv\")\n",
    "train_data = preprocess_data(train_data)\n",
    "\n",
    "# Definiere die Kernelkomponenten\n",
    "kernel_components = [\n",
    "    GPy.kern.StdPeriodic(1, period=10, lengthscale=1.0, variance=1.0), # Täglicher Zyklus (10 Stunden)\n",
    "    GPy.kern.StdPeriodic(1, period=70, lengthscale=1.0, variance=1.0), # Wöchentlicher Zyklus (10 Stunden * 7 Tage)\n",
    "    GPy.kern.Linear(1),\n",
    "    GPy.kern.RBF(1, lengthscale=1.0),\n",
    "]\n",
    "\n",
    "# Optuna-Objektiv-Funktion\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    kernel_list = []\n",
    "    for component in kernel_components:\n",
    "        if trial.suggest_categorical(f\"use_{component.__class__.__name__}\", [0, 1]):\n",
    "            if isinstance(component, GPy.kern.StdPeriodic):\n",
    "                lengthscale = trial.suggest_float(f\"lengthscale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.lengthscale = lengthscale\n",
    "            elif isinstance(component, GPy.kern.RBF):\n",
    "                lengthscale = trial.suggest_float(f\"lengthscale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.lengthscale = lengthscale\n",
    "            kernel_list.append(component)\n",
    "    \n",
    "    print(\"end=\", kernel_list)\n",
    "    kernel = kernel_list[0]\n",
    "    for k in kernel_list[1:]:\n",
    "        kernel += k\n",
    "\n",
    "    gp = GPy.models.GPRegression(X_train, y_train, kernel=kernel)\n",
    "    gp.optimize()\n",
    "    \n",
    "    y_pred, y_var = gp.predict(X_val)\n",
    "    score = -np.mean((y_val - y_pred) ** 2)  # Negatives durchschnittliches quadratisches Fehler\n",
    "    return score\n",
    "\n",
    "\n",
    "# Erstelle mehrere Train-Validierungs-Splits\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "# Finde den Startmonat und das Startjahr\n",
    "start_year = train_data['date'].dt.year.min()\n",
    "start_month = train_data['date'][train_data['date'].dt.year == start_year].dt.month.min()\n",
    "\n",
    "# Berechne den absoluten Monat für jeden Eintrag\n",
    "train_data['abs_month'] = (train_data['date'].dt.year - start_year) * 12 + (train_data['date'].dt.month - start_month)\n",
    "\n",
    "num_splits = 5\n",
    "train_val_splits = []\n",
    "\n",
    "for i in range(num_splits):\n",
    "    # Da wir die ersten 19 Monate nutzen wollen, adjustiere die Grenzen entsprechend\n",
    "    train_months_end = 18 + i # Der 19. Monat (0-basiert)\n",
    "    val_month = 19 + i  # Der Validierungsmonat folgt auf die ersten 19 Monate\n",
    "\n",
    "    # Filtere die Daten\n",
    "    train_data_split = train_data[train_data['abs_month'] <= train_months_end]\n",
    "    val_data_split = train_data[train_data['abs_month'] == val_month]\n",
    "\n",
    "    # Entferne die Hilfsspalten für die Aufteilung\n",
    "    train_data_split = train_data_split.drop(columns=[\"date\", \"abs_month\"])\n",
    "    val_data_split = val_data_split.drop(columns=[\"date\", \"abs_month\"])\n",
    "\n",
    "    # Teile die Daten in Features und Zielvariable\n",
    "    X_train = train_data_split.drop(columns=[\"revenue\"])\n",
    "    y_train = train_data_split[\"revenue\"]\n",
    "    X_val = val_data_split.drop(columns=[\"revenue\"])\n",
    "    y_val = val_data_split[\"revenue\"]\n",
    "\n",
    "    train_val_splits.append((X_train, y_train, X_val, y_val))\n",
    "\n",
    "# Führe die Optimierung mit Optuna durch\n",
    "best_params = None\n",
    "best_score = -np.inf\n",
    "for X_train, y_train, X_val, y_val in train_val_splits:\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=100, n_jobs=1)\n",
    "    \n",
    "    score = study.best_value\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = study.best_trial.params\n",
    "\n",
    "# Extrahiere die besten Hyperparameter und Kernelzusammensetzung\n",
    "# kernel_list = []\n",
    "# for component in kernel_components:\n",
    "#     if best_params.get(f\"use_{component.__class__.__name__}\", 0):\n",
    "#         component_params = {param: best_params[param] for param in best_params if param.startswith(f\"{component.__class__.__name__}\")}\n",
    "#         component.set_params(**component_params)\n",
    "#         kernel_list.append(component)\n",
    "\n",
    "# kernel = GPy.kern.Sum(*kernel_list)\n",
    "\n",
    "# # Trainiere das GP-Modell mit dem optimierten Kernel\n",
    "# gp = GPy.models.GPRegression(X_train, y_train, kernel=kernel)\n",
    "# gp.optimize()\n",
    "\n",
    "# # Mache Vorhersagen\n",
    "# y_pred, y_var = gp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
