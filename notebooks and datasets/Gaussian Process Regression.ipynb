{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimierung von GPR, hier gab es einige Probleme unter anderem mit inkompatiblen Bibliotheken, wodurch diese in einem seperaten Repo lokal zuende geführt wurde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_data(data):    \n",
    "    # Entferne aus den Trainingsdaten die Zeilen, die um 23 Uhr sind, weil wir nur an Samstag Sonntag bis 23 Uhr offen haben, das aber unsere\n",
    "    # Periodizität kaputt macht\n",
    "    data = data[data[\"hour\"] != 23]\n",
    "\n",
    "    #Entferne die Spalten \"year\", \"week\", \"coronaImpact\", weil diese keinen Impact haben\n",
    "    data = data.drop(columns=[\"year\", \"week\", \"coronaImpact\"])\n",
    "\n",
    "    # Sinus and Cosinus Transformation for \"hour\" and \"dayOfMonth\"\n",
    "    # data[\"hour_sin\"] = np.sin(2 * np.pi * data[\"hour\"] / 24)\n",
    "    # data[\"hour_cos\"] = np.cos(2 * np.pi * data[\"hour\"] / 24)\n",
    "    # data[\"dayOfMonth_sin\"] = np.sin(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    # data[\"dayOfMonth_cos\"] = np.cos(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    # data = data.drop(columns=[\"hour\", \"dayOfMonth\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "def split_train_test_data(data, target_col=\"revenue\"):\n",
    "    # date in datetime umwandeln\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "\n",
    "    # Auswählen des Testdatensatzes (letzter Monat)\n",
    "    last_month = data['date'].dt.month.max()\n",
    "    test_data = data[data['date'].dt.month == last_month]\n",
    "    train_data = data[data['date'].dt.month != last_month]\n",
    "\n",
    "    # Reduziere die Trainingsdaten auf die letzten 2 Jahre\n",
    "    last_date = train_data[\"date\"].max()\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    last_date = last_date - pd.DateOffset(years=2)\n",
    "    train_data = train_data[pd.to_datetime(train_data.date) > last_date]\n",
    "\n",
    "    # Entferne Spalte \"date\"\n",
    "    train_data = train_data.drop(columns=[\"date\"])\n",
    "    test_data = test_data.drop(columns=[\"date\"])\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "    # x, y trennen\n",
    "    # X_train = train_data.drop(columns=[target_col])\n",
    "    # y_train = train_data[target_col]\n",
    "    # X_test = test_data.drop(columns=[target_col])\n",
    "    # y_test = test_data[target_col]\n",
    "\n",
    "    # return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data_till_2023.csv\")\n",
    "\n",
    "train_data = preprocess_data(train_data)\n",
    "train_data.to_csv(\"train_data_till_2023_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared, DotProduct\n",
    "import optuna\n",
    "\n",
    "# Vorverarbeitung deiner Daten\n",
    "X_train = np.column_stack([train_data['day_x_hour'], train_data['hour_sin'], train_data['hour_cos'], \n",
    "                           train_data['dayOfMonth_sin'], train_data['dayOfMonth_cos']])\n",
    "y_train = (train_data['revenue'] - train_data['revenue'].mean()) / train_data['revenue'].std()\n",
    "\n",
    "# Definiere die Kernelkomponenten\n",
    "kernel_components = [\n",
    "    RBF(length_scale=1.0, length_scale_bounds=(1e-5, 100.0)),\n",
    "    WhiteKernel(noise_level=1.0, noise_level_bounds=(1e-5, 100.0)),\n",
    "    RationalQuadratic(length_scale=1.0, alpha=1.0, length_scale_bounds=(1e-5, 100.0), alpha_bounds=(1e-5, 100.0)),\n",
    "    ExpSineSquared(length_scale=1.0, periodicity=1.0, length_scale_bounds=(1e-5, 100.0), periodicity_bounds=(1e-5, 100.0)),\n",
    "    DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-5, 100.0))\n",
    "]\n",
    "\n",
    "# Optuna-Objektiv-Funktion\n",
    "def objective(trial):\n",
    "    kernel = 1.0\n",
    "    for component in kernel_components:\n",
    "        if trial.suggest_categorical(f\"use_{component.__class__.__name__}\", [0, 1]):\n",
    "            if isinstance(component, RBF):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(length_scale=length_scale)\n",
    "            elif isinstance(component, WhiteKernel):\n",
    "                noise_level = trial.suggest_float(f\"noise_level_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(noise_level=noise_level)\n",
    "            elif isinstance(component, RationalQuadratic):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                alpha = trial.suggest_float(f\"alpha_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(length_scale=length_scale, alpha=alpha)\n",
    "            elif isinstance(component, ExpSineSquared):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                periodicity = trial.suggest_float(f\"periodicity_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(length_scale=length_scale, periodicity=periodicity)\n",
    "            elif isinstance(component, DotProduct):\n",
    "                sigma_0 = trial.suggest_float(f\"sigma_0_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(sigma_0=sigma_0)\n",
    "            kernel *= component\n",
    "        \n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=True)\n",
    "    gp.fit(X_train, y_train)\n",
    "    \n",
    "    X_val = np.column_stack([val_data['day_x_hour'], val_data['hour_sin'], val_data['hour_cos'],\n",
    "                             val_data['dayOfMonth_sin'], val_data['dayOfMonth_cos']])\n",
    "    y_val = (val_data['revenue'] - val_data['revenue'].mean()) / val_data['revenue'].std()\n",
    "    \n",
    "    y_pred, _ = gp.predict(X_val, return_std=False)\n",
    "    score = -np.mean((y_val - y_pred) ** 2)  # Negatives durchschnittliches quadratisches Fehler\n",
    "    return score\n",
    "\n",
    "# Führe die Optimierung mit Optuna durch\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Extrahiere die besten Hyperparameter und Kernelzusammensetzung\n",
    "best_params = study.best_trial.params\n",
    "kernel = 1.0\n",
    "for component in kernel_components:\n",
    "    if best_params.get(f\"use_{component.__class__.__name__}\", 0):\n",
    "        component_params = {param: best_params[param] for param in best_params if param.startswith(f\"{component.__class__.__name__}\")}\n",
    "        component.set_params(**component_params)\n",
    "        kernel *= component\n",
    "\n",
    "# Trainiere das endgültige Modell mit den besten Hyperparametern\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=True)\n",
    "gp.fit(X_train, y_train)\n",
    "\n",
    "# Mache Vorhersagen auf Testdaten\n",
    "X_test = np.column_stack([test_data['day_x_hour'], test_data['hour_sin'], test_data['hour_cos'],\n",
    "                          test_data['dayOfMonth_sin'], test_data['dayOfMonth_cos']])\n",
    "y_pred, _ = gp.predict(X_test, return_std=False)\n",
    "y_pred = y_pred * test_data['revenue'].std() + test_data['revenue'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "from scipy.stats import norm, gamma\n",
    "\n",
    "# Definiere die Kernel-Komposition\n",
    "kernel = GPy.kern.Sum(\n",
    "    GPy.kern.Periodic(1, period=10, lengthscale=1.0, lower=1e-5, upper=100.0),  # Täglicher Zyklus (10 Stunden)\n",
    "    GPy.kern.Periodic(1, period=70, lengthscale=1.0, lower=1e-5, upper=100.0), # Wöchentlicher Zyklus (10 Stunden * 7 Tage)\n",
    "    GPy.kern.Linear(),  \n",
    "    GPy.kern.RBF(1, lengthscale=1.0, lower=1e-5, upper=100.0),\n",
    "    GPy.kern.Spectral1D(1, lengthscale=1.0, lower=1e-5, upper=100.0), \n",
    "    GPy.kern.Spectral1D(1, lengthscale=1.0, lower=1e-5, upper=100.0)\n",
    ")\n",
    "\n",
    "num_timeseries = len(train_data) // 100 * 3 # Verwende z.B. 3% der Daten für das hierarchische Modell\n",
    "with pm.Model() as hier_model:\n",
    "    # Hyperpriors\n",
    "    nu_s = pm.Normal('nu_s', mu=0, sigma=5)\n",
    "    nu_r = pm.Normal('nu_r', mu=0, sigma=5)\n",
    "    nu_p1 = pm.Normal('nu_p1', mu=0, sigma=5)\n",
    "    nu_p2 = pm.Normal('nu_p2', mu=0, sigma=5)\n",
    "    nu_m1 = pm.Normal('nu_m1', mu=-1.5, sigma=5)\n",
    "    nu_m2 = pm.Normal('nu_m2', mu=0, sigma=5)\n",
    "    lambda_s = pm.Gamma('lambda_s', alpha=1, beta=1)\n",
    "    lambda_l = pm.Gamma('lambda_l', alpha=1, beta=1)\n",
    "\n",
    "    # Hierarchical priors\n",
    "    sigma_s = pm.LogNormal('sigma_s', nu_s, lambda_s, shape=num_timeseries)\n",
    "    l_rbf = pm.LogNormal('l_rbf', nu_r, lambda_l, shape=num_timeseries)\n",
    "    l_per1 = pm.LogNormal('l_per1', nu_p1, lambda_l, shape=num_timeseries)\n",
    "    l_per2 = pm.LogNormal('l_per2', nu_p2, lambda_l, shape=num_timeseries)\n",
    "    l_sm1 = pm.LogNormal('l_sm1', nu_m1, lambda_l, shape=num_timeseries)\n",
    "    l_sm2 = pm.LogNormal('l_sm2', nu_m2, lambda_l, shape=num_timeseries)\n",
    "\n",
    "    # Likelihood (Beispiel für eine Normalverteilung)\n",
    "    y_obs = pm.Normal('y_obs', mu=0, sigma=sigma_s, observed=y_train)\n",
    "\n",
    "    # Inferenz\n",
    "    trace = pm.sample(1000, chains=2, cores=2)\n",
    "\n",
    "# Extrahiere die a-posteriori Verteilungen der Hyperparameter\n",
    "nu_s_post = trace['nu_s'].mean()\n",
    "nu_r_post = trace['nu_r'].mean()\n",
    "nu_p1_post = trace['nu_p1'].mean()\n",
    "nu_p2_post = trace['nu_p2'].mean()\n",
    "nu_m1_post = trace['nu_m1'].mean()\n",
    "nu_m2_post = trace['nu_m2'].mean()\n",
    "lambda_s_post = trace['lambda_s'].mean()\n",
    "lambda_l_post = trace['lambda_l'].mean()\n",
    "\n",
    "print(nu_s_post, nu_r_post, nu_p1_post, nu_p2_post, nu_m1_post, nu_m2_post, lambda_s_post, lambda_l_post)\n",
    "\n",
    "# Erstelle das GP-Modell mit den Priors\n",
    "gp = GPy.models.GPRegression(X_train, y_train, kernel=kernel)\n",
    "gp.kern.rbf.lengthscale.unconstrain()\n",
    "gp.kern.periodic.lengthscales.unconstrain()\n",
    "gp.kern.spectral1d_1.lengthscale.unconstrain()\n",
    "gp.kern.spectral1d_2.lengthscale.unconstrain()\n",
    "\n",
    "# Setze die Priors mit den geschätzten Werten\n",
    "gp.kern.rbf.lengthscale.set_prior(norm(nu_r_post, lambda_l_post))\n",
    "gp.kern.periodic.lengthscales.set_prior(norm([nu_p1_post, nu_p2_post], lambda_l_post))\n",
    "gp.kern.spectral1d_1.lengthscale .set_prior(norm(nu_m1_post, lambda_l_post))\n",
    "gp.kern.spectral1d_2.lengthscale.set_prior(norm(nu_m2_post, lambda_l_post))\n",
    "\n",
    "gp.kern.rbf.variance.set_prior(gamma(shape=lambda_s_post, scale=1/nu_s_post))\n",
    "gp.kern.periodic.variance.set_prior(gamma(shape=lambda_s_post, scale=1/nu_s_post))\n",
    "gp.kern.linear.variances.set_prior(gamma(shape=lambda_s_post, scale=1/nu_s_post))\n",
    "gp.kern.spectral1d_1.variance.set_prior(gamma(shape=lambda_s_post, scale=1/nu_s_post))\n",
    "gp.kern.spectral1d_2.variance.set_prior(gamma(shape=lambda_s_post, scale=1/nu_s_post))\n",
    "\n",
    "# Trainiere das GP-Modell\n",
    "gp.optimize()\n",
    "\n",
    "# Mache Vorhersagen\n",
    "y_pred, y_var = gp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Speichere das Kernel-Objekt\n",
    "with open('optimized_kernel.pkl', 'wb') as file:\n",
    "    pickle.dump(gp.kern, file)\n",
    "\n",
    "\n",
    "print(\"RBF Kernel:\")\n",
    "print(f\"Lengthscale: {gp.kern.rbf.lengthscale.values}\")\n",
    "print(f\"Variance: {gp.kern.rbf.variance.values}\")\n",
    "\n",
    "print(\"Periodischer Kernel 1 (täglich):\")\n",
    "print(f\"Periode: {gp.kern.periodic.period}\")  \n",
    "print(f\"Lengthscale: {gp.kern.periodic.lengthscale.values}\")\n",
    "print(f\"Variance: {gp.kern.periodic.variance.values}\")\n",
    "print(\"Periodischer Kernel 2 (wöchentlich):\")\n",
    "print(f\"Periode: {gp.kern.periodic.period}\")\n",
    "print(f\"Lengthscale: {gp.kern.periodic.lengthscale.values}\")\n",
    "print(f\"Variance: {gp.kern.periodic.variance.values}\")\n",
    "print(\"Linear Kernel:\")\n",
    "print(f\"Variances: {gp.kern.linear.variances.values}\")\n",
    "print(\"Spectral Kernel 1:\")\n",
    "print(f\"Lengthscale: {gp.kern.spectral1d_1.lengthscale.values}\")\n",
    "print(f\"Variance: {gp.kern.spectral1d_1.variance.values}\")\n",
    "print(\"Spectral Kernel 2:\")\n",
    "print(f\"Lengthscale: {gp.kern.spectral1d_2.lengthscale.values}\")\n",
    "\n",
    "\n",
    "# Laden Sie es später wieder ein\n",
    "# with open('optimized_kernel.pkl', 'rb') as file:\n",
    "#     optimized_kernel = pickle.load(file)\n",
    "\n",
    "# # Verwenden Sie das geladene Kernel in einem neuen GPy-Modell\n",
    "# new_gp = GPy.models.GPRegression(X_train, y_train, kernel=optimized_kernel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared, DotProduct\n",
    "import optuna\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Reduziere die Trainingsdaten auf die letzten 4 Jahre\n",
    "    last_date = data[\"date\"].max()\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    last_date = last_date - pd.DateOffset(years=4)\n",
    "    data = data[pd.to_datetime(data.date) > last_date]\n",
    "\n",
    "    \n",
    "    # Entferne aus den Trainingsdaten die Zeilen, die um 23 Uhr sind, weil wir nur an Samstag Sonntag bis 23 Uhr offen haben, das aber unsere\n",
    "    # Periodizität kaputt macht\n",
    "    data = data[data[\"hour\"] != 23]\n",
    "\n",
    "    #Entferne die Spalten \"year\", \"week\", \"coronaImpact\", weil diese keinen Impact haben\n",
    "    data = data.drop(columns=[\"year\", \"week\", \"coronaImpact\"])\n",
    "\n",
    "    # Sinus and Cosinus Transformation for \"hour\" and \"dayOfMonth\"\n",
    "    data[\"hour_sin\"] = np.sin(2 * np.pi * data[\"hour\"] / 24)\n",
    "    data[\"hour_cos\"] = np.cos(2 * np.pi * data[\"hour\"] / 24)\n",
    "    data[\"dayOfMonth_sin\"] = np.sin(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    data[\"dayOfMonth_cos\"] = np.cos(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    data = data.drop(columns=[\"hour\", \"dayOfMonth\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Vorverarbeitung deiner Daten\n",
    "train_data = pd.read_csv(\"train_data_till_2023.csv\")\n",
    "train_data = preprocess_data(train_data)\n",
    "\n",
    "# Definiere die Kernelkomponenten\n",
    "kernel_components = [\n",
    "    # RBF(length_scale=1.0, length_scale_bounds=(1e-5, 100.0)),\n",
    "    WhiteKernel(noise_level=1.0, noise_level_bounds=(1e-5, 100.0)),\n",
    "    RationalQuadratic(length_scale=1.0, alpha=1.0, length_scale_bounds=(1e-5, 100.0), alpha_bounds=(1e-5, 100.0)),\n",
    "    ExpSineSquared(length_scale=1.0, periodicity=10.0, length_scale_bounds=(1e-5, 100.0), periodicity_bounds=(1e-5, 100.0)),\n",
    "    ExpSineSquared(length_scale=1.0, periodicity=70.0, length_scale_bounds=(1e-5, 100.0), periodicity_bounds=(1e-5, 100.0)),\n",
    "    DotProduct(sigma_0=1.0, sigma_0_bounds=(1e-5, 100.0))\n",
    "]\n",
    "\n",
    "# Optuna-Objektiv-Funktion\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    kernel = 1.0\n",
    "    for component in kernel_components:\n",
    "        if trial.suggest_categorical(f\"use_{component.__class__.__name__}\", [0, 1]):\n",
    "            if isinstance(component, RBF):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(length_scale=length_scale)\n",
    "            elif isinstance(component, WhiteKernel):\n",
    "                noise_level = trial.suggest_float(f\"noise_level_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(noise_level=noise_level)\n",
    "            elif isinstance(component, RationalQuadratic):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                alpha = trial.suggest_float(f\"alpha_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(length_scale=length_scale, alpha=alpha)\n",
    "            elif isinstance(component, ExpSineSquared):\n",
    "                length_scale = trial.suggest_float(f\"length_scale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                periodicity = component.periodicity  # Keine Optimierung der Periodizität\n",
    "                component.set_params(length_scale=length_scale)\n",
    "            elif isinstance(component, DotProduct):\n",
    "                sigma_0 = trial.suggest_float(f\"sigma_0_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.set_params(sigma_0=sigma_0)\n",
    "            kernel *= component\n",
    "        \n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0)\n",
    "    gp.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gp.predict(X_val, return_std=False)\n",
    "    score = -np.mean((y_val - y_pred) ** 2)  # Negatives durchschnittliches quadratisches Fehler\n",
    "    return score\n",
    "\n",
    "# Erstelle mehrere Train-Validierungs-Splits\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "# Finde den Startmonat und das Startjahr\n",
    "start_year = train_data['date'].dt.year.min()\n",
    "start_month = train_data['date'][train_data['date'].dt.year == start_year].dt.month.min()\n",
    "\n",
    "# Berechne den absoluten Monat für jeden Eintrag\n",
    "train_data['abs_month'] = (train_data['date'].dt.year - start_year) * 12 + (train_data['date'].dt.month - start_month)\n",
    "\n",
    "num_splits = 5\n",
    "train_val_splits = []\n",
    "\n",
    "for i in range(num_splits):\n",
    "    # Da wir die ersten 19 Monate nutzen wollen, adjustiere die Grenzen entsprechend\n",
    "    train_months_end = 18 + i # Der 19. Monat (0-basiert)\n",
    "    val_month = 19 + i  # Der Validierungsmonat folgt auf die ersten 19 Monate\n",
    "\n",
    "    # Filtere die Daten\n",
    "    train_data_split = train_data[train_data['abs_month'] <= train_months_end]\n",
    "    val_data_split = train_data[train_data['abs_month'] == val_month]\n",
    "\n",
    "    # Entferne die Hilfsspalten für die Aufteilung\n",
    "    train_data_split = train_data_split.drop(columns=[\"date\", \"abs_month\"])\n",
    "    val_data_split = val_data_split.drop(columns=[\"date\", \"abs_month\"])\n",
    "\n",
    "    # Teile die Daten in Features und Zielvariable\n",
    "    X_train = train_data_split.drop(columns=[\"revenue\"])\n",
    "    y_train = train_data_split[\"revenue\"]\n",
    "    X_val = val_data_split.drop(columns=[\"revenue\"])\n",
    "    y_val = val_data_split[\"revenue\"]\n",
    "\n",
    "    train_val_splits.append((X_train, y_train, X_val, y_val))\n",
    "\n",
    "# Führe die Optimierung mit Optuna durch\n",
    "best_params = None\n",
    "best_score = -np.inf\n",
    "for X_train, y_train, X_val, y_val in train_val_splits:\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=100, n_jobs=1)\n",
    "    \n",
    "    score = study.best_value\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = study.best_trial.params\n",
    "\n",
    "# Extrahiere die besten Hyperparameter und Kernelzusammensetzung\n",
    "kernel = 1.0\n",
    "for component in kernel_components:\n",
    "    if best_params.get(f\"use_{component.__class__.__name__}\", 0):\n",
    "        component_params = {param: best_params[param] for param in best_params if param.startswith(f\"{component.__class__.__name__}\")}\n",
    "        component.set_params(**component_params)\n",
    "        kernel *= component\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gebe die besten Hyperparameter und Kernelzusammensetzung aus\n",
    "print(\"Beste Hyperparameter:\")\n",
    "print(best_params)\n",
    "print(\"Beste Kernelzusammensetzung:\")\n",
    "print(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Trainiere das endgültige Modell mit den besten Hyperparametern\n",
    "# X_train = np.column_stack([train_data['day_x_hour'], train_data['hour_sin'], train_data['hour_cos'],\n",
    "#                            train_data['dayOfMonth_sin'], train_data['dayOfMonth_cos']])\n",
    "# y_train = (train_data['revenue'] - train_data['revenue'].mean()) / train_data['revenue'].std()\n",
    "\n",
    "# gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=True)\n",
    "# gp.fit(X_train, y_train)\n",
    "\n",
    "# # Mache Vorhersagen auf Testdaten\n",
    "# test_data = train_data[train_data['date'].dt.month == train_data['date'].dt.month.max()]\n",
    "# X_test = np.column_stack([test_data['day_x_hour'], test_data['hour_sin'], test_data['hour_cos'],\n",
    "#                           test_data['dayOfMonth_sin'], test_data['dayOfMonth_cos']])\n",
    "# y_pred, _ = gp.predict(X_test, return_std=False)\n",
    "# y_pred = y_pred * test_data['revenue'].std() + test_data['revenue'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import GPy\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Reduziere die Trainingsdaten auf die letzten 4 Jahre\n",
    "    last_date = data[\"date\"].max()\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    last_date = last_date - pd.DateOffset(months=25)\n",
    "    data = data[pd.to_datetime(data.date) > last_date]\n",
    "\n",
    "    \n",
    "    # Entferne aus den Trainingsdaten die Zeilen, die um 23 Uhr sind, weil wir nur an Samstag Sonntag bis 23 Uhr offen haben, das aber unsere\n",
    "    # Periodizität kaputt macht\n",
    "    data = data[data[\"hour\"] != 23]\n",
    "\n",
    "    #Entferne die Spalten \"year\", \"week\", \"coronaImpact\", weil diese keinen Impact haben\n",
    "    data = data.drop(columns=[\"year\", \"week\", \"coronaImpact\"])\n",
    "\n",
    "    # Sinus and Cosinus Transformation for \"hour\" and \"dayOfMonth\"\n",
    "    data[\"hour_sin\"] = np.sin(2 * np.pi * data[\"hour\"] / 24)\n",
    "    data[\"hour_cos\"] = np.cos(2 * np.pi * data[\"hour\"] / 24)\n",
    "    data[\"dayOfMonth_sin\"] = np.sin(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    data[\"dayOfMonth_cos\"] = np.cos(2 * np.pi * data[\"dayOfMonth\"] / 31)\n",
    "    data = data.drop(columns=[\"hour\", \"dayOfMonth\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Vorverarbeitung deiner Daten\n",
    "train_data = pd.read_csv(\"train_data_till_2023.csv\")\n",
    "train_data = preprocess_data(train_data)\n",
    "\n",
    "# Definiere die Kernelkomponenten\n",
    "kernel_components = [\n",
    "    GPy.kern.StdPeriodic(1, period=10, lengthscale=1.0, variance=1.0), # Täglicher Zyklus (10 Stunden)\n",
    "    GPy.kern.StdPeriodic(1, period=70, lengthscale=1.0, variance=1.0), # Wöchentlicher Zyklus (10 Stunden * 7 Tage)\n",
    "    GPy.kern.Linear(1),\n",
    "    GPy.kern.RBF(1, lengthscale=1.0),\n",
    "]\n",
    "\n",
    "# Optuna-Objektiv-Funktion\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    kernel_list = []\n",
    "    for component in kernel_components:\n",
    "        if trial.suggest_categorical(f\"use_{component.__class__.__name__}\", [0, 1]):\n",
    "            if isinstance(component, GPy.kern.StdPeriodic):\n",
    "                lengthscale = trial.suggest_float(f\"lengthscale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.lengthscale = lengthscale\n",
    "            elif isinstance(component, GPy.kern.RBF):\n",
    "                lengthscale = trial.suggest_float(f\"lengthscale_{component.__class__.__name__}\", 1e-5, 100.0, log=True)\n",
    "                component.lengthscale = lengthscale\n",
    "            kernel_list.append(component)\n",
    "    \n",
    "    print(\"end=\", kernel_list)\n",
    "    kernel = kernel_list[0]\n",
    "    for k in kernel_list[1:]:\n",
    "        kernel += k\n",
    "\n",
    "    gp = GPy.models.GPRegression(X_train, y_train, kernel=kernel)\n",
    "    gp.optimize()\n",
    "    \n",
    "    y_pred, y_var = gp.predict(X_val)\n",
    "    score = -np.mean((y_val - y_pred) ** 2)  # Negatives durchschnittliches quadratisches Fehler\n",
    "    return score\n",
    "\n",
    "\n",
    "# Erstelle mehrere Train-Validierungs-Splits\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "# Finde den Startmonat und das Startjahr\n",
    "start_year = train_data['date'].dt.year.min()\n",
    "start_month = train_data['date'][train_data['date'].dt.year == start_year].dt.month.min()\n",
    "\n",
    "# Berechne den absoluten Monat für jeden Eintrag\n",
    "train_data['abs_month'] = (train_data['date'].dt.year - start_year) * 12 + (train_data['date'].dt.month - start_month)\n",
    "\n",
    "num_splits = 5\n",
    "train_val_splits = []\n",
    "\n",
    "for i in range(num_splits):\n",
    "    # Da wir die ersten 19 Monate nutzen wollen, adjustiere die Grenzen entsprechend\n",
    "    train_months_end = 18 + i # Der 19. Monat (0-basiert)\n",
    "    val_month = 19 + i  # Der Validierungsmonat folgt auf die ersten 19 Monate\n",
    "\n",
    "    # Filtere die Daten\n",
    "    train_data_split = train_data[train_data['abs_month'] <= train_months_end]\n",
    "    val_data_split = train_data[train_data['abs_month'] == val_month]\n",
    "\n",
    "    # Entferne die Hilfsspalten für die Aufteilung\n",
    "    train_data_split = train_data_split.drop(columns=[\"date\", \"abs_month\"])\n",
    "    val_data_split = val_data_split.drop(columns=[\"date\", \"abs_month\"])\n",
    "\n",
    "    # Teile die Daten in Features und Zielvariable\n",
    "    X_train = train_data_split.drop(columns=[\"revenue\"])\n",
    "    y_train = train_data_split[\"revenue\"]\n",
    "    X_val = val_data_split.drop(columns=[\"revenue\"])\n",
    "    y_val = val_data_split[\"revenue\"]\n",
    "\n",
    "    train_val_splits.append((X_train, y_train, X_val, y_val))\n",
    "\n",
    "# Führe die Optimierung mit Optuna durch\n",
    "best_params = None\n",
    "best_score = -np.inf\n",
    "for X_train, y_train, X_val, y_val in train_val_splits:\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=100, n_jobs=1)\n",
    "    \n",
    "    score = study.best_value\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = study.best_trial.params\n",
    "\n",
    "# Extrahiere die besten Hyperparameter und Kernelzusammensetzung\n",
    "# kernel_list = []\n",
    "# for component in kernel_components:\n",
    "#     if best_params.get(f\"use_{component.__class__.__name__}\", 0):\n",
    "#         component_params = {param: best_params[param] for param in best_params if param.startswith(f\"{component.__class__.__name__}\")}\n",
    "#         component.set_params(**component_params)\n",
    "#         kernel_list.append(component)\n",
    "\n",
    "# kernel = GPy.kern.Sum(*kernel_list)\n",
    "\n",
    "# # Trainiere das GP-Modell mit dem optimierten Kernel\n",
    "# gp = GPy.models.GPRegression(X_train, y_train, kernel=kernel)\n",
    "# gp.optimize()\n",
    "\n",
    "# # Mache Vorhersagen\n",
    "# y_pred, y_var = gp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
